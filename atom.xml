<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Site</title>
  
  
  <link href="https://lwdavid.github.io/atom.xml" rel="self"/>
  
  <link href="https://lwdavid.github.io/"/>
  <updated>2023-09-09T15:14:53.930Z</updated>
  <id>https://lwdavid.github.io/</id>
  
  <author>
    <name>David</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>《机器学习方法》 第1章 机器学习及监督学习概论</title>
    <link href="https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC1%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"/>
    <id>https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC1%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/</id>
    <published>2023-09-09T12:56:07.000Z</published>
    <updated>2023-09-09T15:14:53.930Z</updated>
    
    <content type="html"><![CDATA[<h1 id="11-统计学习"><a class="markdownIt-Anchor" href="#11-统计学习"></a> 1.1 统计学习</h1><p>学习的定义:</p><blockquote><p>Learning is any process by which a system improves performance from experience.<br />——Herbert A. Simon</p></blockquote><p>机器学习的对象是数据。<br />机器学习由监督学习、无监督学习和强化学习等组成。<br />机器学习的三要素是：模型、策略、算法。<br />实现机器学习方法的步骤是：获得数据、确定模型、选择策略、实现算法、选择模型、进行预测或分析。</p><h1 id="12-机器学习的分类"><a class="markdownIt-Anchor" href="#12-机器学习的分类"></a> 1.2 机器学习的分类</h1><h2 id="基本分类监督学习-无监督学习-强化学习-半监督学习与主动学习"><a class="markdownIt-Anchor" href="#基本分类监督学习-无监督学习-强化学习-半监督学习与主动学习"></a> 基本分类：监督学习、无监督学习、强化学习、半监督学习与主动学习。</h2><blockquote><p>给出了输入空间、特征空间、输出空间、实例、特征向量、联合概率分布、假设空间、状态、奖励、动作、策略、价值函数等概念的定义。<br />强化学习有基于策略的、基于价值的，这两种属于无模型的方法（求解策略或价值函数），此外还有有模型的方法（建模并学习问题）。</p></blockquote><h2 id="按模型分类概率与非概率模型-线性与非线性模型-参数化与非参数化模型"><a class="markdownIt-Anchor" href="#按模型分类概率与非概率模型-线性与非线性模型-参数化与非参数化模型"></a> 按模型分类：概率与非概率模型、线性与非线性模型、参数化与非参数化模型。</h2><blockquote><p>概率：决策树、朴素贝叶斯、隐马尔可夫、CRF、概率潜在语义分析、潜在狄利克雷分配、GMM。<br />非概率：感知机、SVM、k近邻、AdaBoost、k均值、潜在语义分析、神经网络。</p></blockquote><blockquote><p>线性：感知机、SVM、k近邻、k均值、潜在语义分析。<br />非线性：核函数SVM、AdaBoost、神经网络。</p></blockquote><blockquote><p>参数化：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、GMM。<br />非参数化：决策树、SVM、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配。</p></blockquote><h2 id="按算法分类在线学习-批量学习"><a class="markdownIt-Anchor" href="#按算法分类在线学习-批量学习"></a> 按算法分类：在线学习、批量学习</h2><p>数据依次到达无法存储、系统要求及时处理、数据规模很大、模式动态变化等情况需要考虑在线学习。</p><h2 id="按技巧分类贝叶斯学习-核方法"><a class="markdownIt-Anchor" href="#按技巧分类贝叶斯学习-核方法"></a> 按技巧分类：贝叶斯学习、核方法</h2><blockquote><p>贝叶斯方法：主要利用贝叶斯定理。即根据获得的数据基于后验概率去估计模型，指导模型的选择。需要选择模型时就选择后验概率最大的模型，预测时就计算整个后验概率分布上的期望。和最大似然估计的差别在于，极大似然估计直接选取后验概率最大的模型和预测，贝叶斯方法则在全体上取期望。<br />核方法：不显式地定义低维到高维的映射，而是直接定义映射后在特征空间的内积，也就是核函数。例如{\it x}_{\rm 1}和{\it x}_{\rm 2}的内积是\left<{\it x}_{\rm 1},{\it x}_{\rm 2} \right>，它们在特征空间的映射是\varphi({\it x}_{\rm 1})和\varphi({\it x}_{\rm 2})，内积是\left<{\varphi}({\it x}_{\rm 1}),\varphi({\it x}_{\rm 2}) \right>。那么核函数{\it K}({\it x}_{\rm 1},{\it x}_{\rm 2})就定义为{\it K}({\it x}_{\rm 1},{\it x}_{\rm 2})=\left<\varphi({\it x}_{\rm 1}),\varphi({\it x}_{\rm 2}) \right>。</p></blockquote><h1 id="13-机器学习方法的三要素"><a class="markdownIt-Anchor" href="#13-机器学习方法的三要素"></a> 1.3 机器学习方法的三要素</h1><blockquote><p>方法=模型+策略+算法</p></blockquote><h2 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h2><p>假设空间<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">F</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{F}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.09931em;">F</span></span></span></span></span>包含所有可能的模型：\mathcal{F}=\{\it{f}|Y=f(X)\}.</p><h2 id="策略"><a class="markdownIt-Anchor" href="#策略"></a> 策略</h2><p>考虑按照什么准测学习或选择模型。<br />引入损失函数{\it L}({\it Y}, {\it f}({\it X}))和风险函数{\it R}_{\rm exp}({\it f})。</p><blockquote><p>损失函数：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数（对数似然损失函数）等。<br />风险函数：模型的损失函数在整个联合分布{\it P}({\it X}, {\it Y})上的期望即为风险函数，也叫期望损失。</p></blockquote><p>由于整个联合分布无从得知，所以只能使用训练集上的平均损失，也就是经验风险或经验损失{\it R}_{\rm emp}({\it f})。<br />根据大数定理，当样本容量{\it N}\rightarrow\infty时有{\it R}_{\rm emp}({\it f})\rightarrow{\it R}_{\rm exp}({\it f})。<br />监督学习有两个策略：经验风险最小化(Empirical Risk Minimization, ERM)和结构风险最小化(Structural Risk Minimization, SRM)。<br />经验风险最小化：</p>\min_{\it f\in\mathcal{F}} \frac{1}{\it N} \sum^{\it N}_{i=1}{\it L}({\it y_i}, {\it f}({\it x_i}))<p>结构风险最小化：</p>\min_{\it f\in\mathcal{F}} \frac{1}{\it N} \sum^{\it N}_{i=1}{\it L}({\it y_i}, {\it f}({\it x_i}))+\lambda{\it J}({\it f})<p>结构风险最小化在经验风险最小化的基础上添加一个正则化项（罚项）以控制模型的复杂度。</p><h2 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h2><p>即求解最优化问题的具体计算方法。</p><h1 id="14-模型评估与模型选择"><a class="markdownIt-Anchor" href="#14-模型评估与模型选择"></a> 1.4 模型评估与模型选择</h1><blockquote><p>训练误差、测试误差。<br />模型的选择：过拟合（模型复杂度高，测试误差与训练误差重新形成较大差距）。</p></blockquote><h1 id="15-正则化与交叉验证"><a class="markdownIt-Anchor" href="#15-正则化与交叉验证"></a> 1.5 正则化与交叉验证</h1><p>正则化项可采用{\it L}_{\rm p}-范数：</p>\lVert{\it x}\rVert_p=\left(\sum^{n}_{i=1}\lvert{\it x}_{\rm i}\rvert^{p}\right)^{\frac{1}{p}}<p>交叉验证：</p><ul><li>简单交叉验证<br />一部分作为训练集，一部分作为验证集。</li><li>S折交叉验证<br />切分为S个大小相同互不相交的子集，一个作为验证集其余作为训练集，然后重复S次。</li><li>留一交叉验证<br />即S=N的S折交叉验证。</li></ul><h1 id="16-泛化能力"><a class="markdownIt-Anchor" href="#16-泛化能力"></a> 1.6 泛化能力</h1><p>泛化误差是该模型在<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="script">X</mi></mrow><mo>×</mo><mrow><mi mathvariant="script">Y</mi></mrow></mrow><annotation encoding="application/x-tex">\mathcal{X}\times\mathcal{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.78055em;vertical-align:-0.09722em;"></span><span class="base textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.14643em;">X</span></span><span class="mbin">×</span><span class="mord textstyle uncramped"><span class="mord mathcal" style="margin-right:0.08222em;">Y</span></span></span></span></span>上损失函数的期望。<br />对二类分类问题，可以由Hoeffding不等式推导出泛化误差上界。</p><h1 id="17-生成模型与判别模型"><a class="markdownIt-Anchor" href="#17-生成模型与判别模型"></a> 1.7 生成模型与判别模型</h1><p>在监督学习中，</p><ul><li>生成模型：可学习联合概率分布、收敛快、可在样本容量增加时收敛于真实模型、能应对隐变量<br />朴素贝叶斯、隐马尔可夫</li><li>判别模型：直接学习决策函数或条件概率分布、准确率高、可对数据进行抽象、能定义并使用特征从而简化学习问题<br />k近邻、感知机、决策树、逻辑斯谛回归、最大熵模型、SVM、提升方法、CRF等</li></ul><h1 id="18-监督学习应用"><a class="markdownIt-Anchor" href="#18-监督学习应用"></a> 1.8 监督学习应用</h1><h2 id="分类"><a class="markdownIt-Anchor" href="#分类"></a> 分类</h2><p>分类器指标：准确率(Accuracy)。<br />二类分类：精确率(Precision)、召回率(Recall)、<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>分数(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span> score)。<br />常用模型：k近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、SVM、提升方法、贝叶斯网络、神经网络、Winnow等。</p><h2 id="标注"><a class="markdownIt-Anchor" href="#标注"></a> 标注</h2><p>指标与分类类似。<br />常用模型：隐马尔可夫、CRF。</p><h2 id="回归"><a class="markdownIt-Anchor" href="#回归"></a> 回归</h2><p>常用的损失函数是平方损失函数，此时可用最小二乘法(Least Squares)求解。</p><h1 id="习题"><a class="markdownIt-Anchor" href="#习题"></a> 习题</h1><p>1.1<br />答案</p><p>1.2</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;11-统计学习&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#11-统计学习&quot;&gt;&lt;/a&gt; 1.1 统计学习&lt;/h1&gt;
&lt;p&gt;学习的定义:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Learning is any process by wh</summary>
      
    
    
    
    
    <category term="机器学习方法" scheme="https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>test_post_2</title>
    <link href="https://lwdavid.github.io/2023/09/09/test-post-2/"/>
    <id>https://lwdavid.github.io/2023/09/09/test-post-2/</id>
    <published>2023-09-09T10:57:14.000Z</published>
    <updated>2023-09-09T10:57:14.159Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>test_post</title>
    <link href="https://lwdavid.github.io/2023/09/09/test-post/"/>
    <id>https://lwdavid.github.io/2023/09/09/test-post/</id>
    <published>2023-09-09T10:56:27.000Z</published>
    <updated>2023-09-09T10:56:27.406Z</updated>
    
    
    
    
    
  </entry>
  
</feed>
