{"meta":{"title":"Site","subtitle":"","description":"","author":"David","url":"https://lwdavid.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2023-09-09T15:43:03.237Z","updated":"2023-09-09T15:43:03.237Z","comments":false,"path":"/404.html","permalink":"https://lwdavid.github.io/404.html","excerpt":"","text":""},{"title":"About","date":"2023-09-09T15:54:01.021Z","updated":"2023-09-09T15:54:01.021Z","comments":false,"path":"about/index.html","permalink":"https://lwdavid.github.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2023-09-09T15:47:53.563Z","updated":"2023-09-09T15:47:53.563Z","comments":false,"path":"categories/index.html","permalink":"https://lwdavid.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2023-09-09T15:42:03.619Z","updated":"2023-09-09T15:42:03.619Z","comments":false,"path":"tags/index.html","permalink":"https://lwdavid.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《机器学习方法》 第2章 感知机","slug":"机器学习方法-第2章-感知机","date":"2023-09-09T15:51:15.000Z","updated":"2023-09-09T15:53:01.851Z","comments":true,"path":"2023/09/09/机器学习方法-第2章-感知机/","link":"","permalink":"https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC2%E7%AB%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"","text":"2.1 感知机模型 2.2 感知机学习策略 2.3 感知机学习算法 习题","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]},{"title":"《机器学习方法》 第1章 机器学习及监督学习概论","slug":"机器学习方法-第1章-机器学习及监督学习概论","date":"2023-09-09T12:56:07.000Z","updated":"2023-09-09T15:50:08.004Z","comments":true,"path":"2023/09/09/机器学习方法-第1章-机器学习及监督学习概论/","link":"","permalink":"https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC1%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/","excerpt":"","text":"1.1 统计学习 学习的定义: Learning is any process by which a system improves performance from experience. ——Herbert A. Simon 机器学习的对象是数据。 机器学习由监督学习、无监督学习和强化学习等组成。 机器学习的三要素是：模型、策略、算法。 实现机器学习方法的步骤是：获得数据、确定模型、选择策略、实现算法、选择模型、进行预测或分析。 1.2 机器学习的分类 基本分类：监督学习、无监督学习、强化学习、半监督学习与主动学习。 给出了输入空间、特征空间、输出空间、实例、特征向量、联合概率分布、假设空间、状态、奖励、动作、策略、价值函数等概念的定义。 强化学习有基于策略的、基于价值的，这两种属于无模型的方法（求解策略或价值函数），此外还有有模型的方法（建模并学习问题）。 按模型分类：概率与非概率模型、线性与非线性模型、参数化与非参数化模型。 概率：决策树、朴素贝叶斯、隐马尔可夫、CRF、概率潜在语义分析、潜在狄利克雷分配、GMM。 非概率：感知机、SVM、k近邻、AdaBoost、k均值、潜在语义分析、神经网络。 线性：感知机、SVM、k近邻、k均值、潜在语义分析。 非线性：核函数SVM、AdaBoost、神经网络。 参数化：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、GMM。 非参数化：决策树、SVM、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配。 按算法分类：在线学习、批量学习 数据依次到达无法存储、系统要求及时处理、数据规模很大、模式动态变化等情况需要考虑在线学习。 按技巧分类：贝叶斯学习、核方法 贝叶斯方法：主要利用贝叶斯定理。即根据获得的数据基于后验概率去估计模型，指导模型的选择。需要选择模型时就选择后验概率最大的模型，预测时就计算整个后验概率分布上的期望。和最大似然估计的差别在于，极大似然估计直接选取后验概率最大的模型和预测，贝叶斯方法则在全体上取期望。 核方法：不显式地定义低维到高维的映射，而是直接定义映射后在特征空间的内积，也就是核函数。例如x1{\\it x}_{\\rm 1}x1​和x2{\\it x}_{\\rm 2}x2​的内积是&lt;x1,x2&gt;\\left&lt;{\\it x}_{\\rm 1},{\\it x}_{\\rm 2} \\right&gt;⟨x1​,x2​⟩，它们在特征空间的映射是φ(x1)\\varphi({\\it x}_{\\rm 1})φ(x1​)和φ(x2)\\varphi({\\it x}_{\\rm 2})φ(x2​)，内积是&lt;φ(x1),φ(x2)&gt;\\left&lt;{\\varphi}({\\it x}_{\\rm 1}),\\varphi({\\it x}_{\\rm 2}) \\right&gt;⟨φ(x1​),φ(x2​)⟩。那么核函数K(x1,x2){\\it K}({\\it x}_{\\rm 1},{\\it x}_{\\rm 2})K(x1​,x2​)就定义为K(x1,x2)=&lt;φ(x1),φ(x2)&gt;{\\it K}({\\it x}_{\\rm 1},{\\it x}_{\\rm 2})=\\left&lt;\\varphi({\\it x}_{\\rm 1}),\\varphi({\\it x}_{\\rm 2}) \\right&gt;K(x1​,x2​)=⟨φ(x1​),φ(x2​)⟩。 1.3 机器学习方法的三要素 方法=模型+策略+算法 模型 假设空间F\\mathcal{F}F包含所有可能的模型：F={f∣Y=f(X)}\\mathcal{F}=\\{\\it{f}|Y=f(X)\\}F={f∣Y=f(X)}. 策略 考虑按照什么准测学习或选择模型。 引入损失函数L(Y,f(X)){\\it L}({\\it Y}, {\\it f}({\\it X}))L(Y,f(X))和风险函数Rexp(f){\\it R}_{\\rm exp}({\\it f})Rexp​(f)。 损失函数：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数（对数似然损失函数）等。 风险函数：模型的损失函数在整个联合分布P(X,Y){\\it P}({\\it X}, {\\it Y})P(X,Y)上的期望即为风险函数，也叫期望损失。 由于整个联合分布无从得知，所以只能使用训练集上的平均损失，也就是经验风险或经验损失Remp(f){\\it R}_{\\rm emp}({\\it f})Remp​(f)。 根据大数定理，当样本容量N→∞{\\it N}\\rightarrow\\inftyN→∞时有Remp(f)→Rexp(f){\\it R}_{\\rm emp}({\\it f})\\rightarrow{\\it R}_{\\rm exp}({\\it f})Remp​(f)→Rexp​(f)。 监督学习有两个策略：经验风险最小化(Empirical Risk Minimization, ERM)和结构风险最小化(Structural Risk Minimization, SRM)。 经验风险最小化： min⁡f∈F1N∑i=1NL(yi,f(xi))\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i})) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​)) 结构风险最小化： min⁡f∈F1N∑i=1NL(yi,f(xi))+λJ(f)\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i}))+\\lambda{\\it J}({\\it f}) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​))+λJ(f) 结构风险最小化在经验风险最小化的基础上添加一个正则化项（罚项）以控制模型的复杂度。 算法 即求解最优化问题的具体计算方法。 1.4 模型评估与模型选择 训练误差、测试误差。 模型的选择：过拟合（模型复杂度高，测试误差与训练误差重新形成较大差距）。 1.5 正则化与交叉验证 正则化项可采用Lp{\\it L}_{\\rm p}Lp​-范数： ∥x∥p=(∑i=1n∣xi∣p)1p\\lVert{\\it x}\\rVert_p=\\left(\\sum^{n}_{i=1}\\lvert{\\it x}_{\\rm i}\\rvert^{p}\\right)^{\\frac{1}{p}} ∥x∥p​=(i=1∑n​∣xi​∣p)p1​ 交叉验证： 简单交叉验证 一部分作为训练集，一部分作为验证集。 S折交叉验证 切分为S个大小相同互不相交的子集，一个作为验证集其余作为训练集，然后重复S次。 留一交叉验证 即S=N的S折交叉验证。 1.6 泛化能力 泛化误差是该模型在X×Y\\mathcal{X}\\times\\mathcal{Y}X×Y上损失函数的期望。 对二类分类问题，可以由Hoeffding不等式推导出泛化误差上界。 1.7 生成模型与判别模型 在监督学习中， 生成模型：可学习联合概率分布、收敛快、可在样本容量增加时收敛于真实模型、能应对隐变量 朴素贝叶斯、隐马尔可夫 判别模型：直接学习决策函数或条件概率分布、准确率高、可对数据进行抽象、能定义并使用特征从而简化学习问题 k近邻、感知机、决策树、逻辑斯谛回归、最大熵模型、SVM、提升方法、CRF等 1.8 监督学习应用 分类 分类器指标：准确率(Accuracy)。 二类分类：精确率(Precision)、召回率(Recall)、F1F_1F1​分数(F1F_1F1​ score)。 常用模型：k近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、SVM、提升方法、贝叶斯网络、神经网络、Winnow等。 标注 指标与分类类似。 常用模型：隐马尔可夫、CRF。 回归 常用的损失函数是平方损失函数，此时可用最小二乘法(Least Squares)求解。 习题 1.1 答案 1.2","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]}