{"meta":{"title":"Site","subtitle":"","description":"","author":"David","url":"https://lwdavid.github.io","root":"/"},"pages":[{"title":"Tags","date":"2023-09-09T15:42:03.619Z","updated":"2023-09-09T15:42:03.619Z","comments":false,"path":"tags/index.html","permalink":"https://lwdavid.github.io/tags/index.html","excerpt":"","text":""},{"title":"About","date":"2023-09-12T03:35:45.401Z","updated":"2023-09-12T03:35:45.401Z","comments":false,"path":"about/index.html","permalink":"https://lwdavid.github.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2023-09-09T15:47:53.563Z","updated":"2023-09-09T15:47:53.563Z","comments":false,"path":"categories/index.html","permalink":"https://lwdavid.github.io/categories/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2023-09-09T15:43:03.237Z","updated":"2023-09-09T15:43:03.237Z","comments":false,"path":"/404.html","permalink":"https://lwdavid.github.io/404.html","excerpt":"","text":""}],"posts":[{"title":"《机器学习方法》第4章 朴素贝叶斯法","slug":"机器学习方法-第4章-朴素贝叶斯法","date":"2023-09-16T08:55:08.000Z","updated":"2023-09-16T08:58:13.737Z","comments":true,"path":"2023/09/16/机器学习方法-第4章-朴素贝叶斯法/","link":"","permalink":"https://lwdavid.github.io/2023/09/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC4%E7%AB%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/","excerpt":"","text":"4.1 朴素贝叶斯法的学习与分类 基本方法 后验概率最大化的含义 4.2 朴素贝叶斯法的参数估计 极大似然估计 学习与分类算法 贝叶斯估计 习题 4.1 4.2","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]},{"title":"《机器学习方法》第3章 k近邻法","slug":"机器学习方法-第3章-k近邻法","date":"2023-09-13T08:24:09.000Z","updated":"2023-09-16T08:54:12.535Z","comments":true,"path":"2023/09/13/机器学习方法-第3章-k近邻法/","link":"","permalink":"https://lwdavid.github.io/2023/09/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC3%E7%AB%A0-k%E8%BF%91%E9%82%BB%E6%B3%95/","excerpt":"","text":"k{\\it k}k近邻法（k{\\it k}k-nearest neighbor，k{\\it k}k-NN）是一种基本分类与回归方法。由Cover和Hart于1968年提出。 3.1 k{\\it k}k近邻算法 给定训练集。对于一个输入的实例，在训练集上寻找距离该实例最近的k{\\it k}k个实例，找出其中多数所属的类，并将输入实例也分为这个类。 例如，在训练集T{\\it T}T中，找出与实例x{\\it x}x最近邻的k{\\it k}k个点，涵盖这k{\\it k}k个点的x{\\it x}x的邻域记作Nk(x){\\it N_k(x)}Nk​(x)。那么根据多数表决决策规则可将x{\\it x}x分类为y:{\\it y}:y: y=arg⁡max⁡cj∑xi∈Nk(x)I(yi=cj), i=1,2,⋯ ,N, j=1,2,⋯ ,K{\\it y}=\\arg \\max \\limits_{\\it c_j} \\sum_{\\it x_i \\in N_k(x)} {\\it I(y_i=c_j)}, ~~~~{\\it i} = 1,2,\\cdots,{\\it N}, ~~~~ {\\it j} = 1, 2, \\cdots, {\\it K} y=argcj​max​xi​∈Nk​(x)∑​I(yi​=cj​), i=1,2,⋯,N, j=1,2,⋯,K 当k=1{\\it k}=1k=1时，k{\\it k}k近邻算法变为特殊情况的最近邻算法。 k{\\it k}k近邻算法没有显式的学习过程。 3.2 k{\\it k}k近邻模型 模型 k{\\it k}k近邻算法的三个基本要素是：k{\\it k}k值的选择、距离度量、分类决策规则。 当三个基本要素确定后，对于任何一个新的输入实例，它所属的类唯一地确定。 在特征空间中，对每个训练实例点xi{\\it x_i}xi​，距离该点比其他点更近的所有点组成一个区域，叫作单元（cell）。每个实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例xi{\\it x_i}xi​的类yi{\\it y_i}yi​作为其单元中所有点的类标记。 距离度量 特征空间中两个实例点的距离是其相似程度的反映。常用的距离有Lp{\\it L_p}Lp​距离和闵可夫斯基（Minkowski）距离等。 Lp{\\it L_p}Lp​距离的定义为 Lp(xi,xj)=(∑l=1n∣xi(l)−xj(l)∣p)1p, p⩾1{\\it L_p(x_i, x_j)} = \\left(\\sum^{\\it n}_{\\it l={\\rm 1}}|{\\it x^{(l)}_i - x^{(l)}_j}|^{\\it p}\\right)^{\\frac{1}{\\it p}}, ~~~~{\\it p}\\geqslant 1 Lp​(xi​,xj​)=(l=1∑n​∣xi(l)​−xj(l)​∣p)p1​, p⩾1 当p=2{\\it p}=2p=2时称为欧式（Euclidean）距离，当p=1{\\it p}=1p=1时称为曼哈顿（Manhattan）距离，当p=∞{\\it p}=\\inftyp=∞时为各维上的距离的最大值。 k{\\it k}k值的选择 k{\\it k}k值较小时，只有与输入实例近的训练实例才对结果起作用，“学习”的近似误差较小。但如果邻近噪声则预测出错，故“学习”的估计误差较大。 换句话说，较小的k{\\it k}k值意味着更复杂的模型，于是更容易过拟合。较大的k{\\it k}k值则使得较远的训练实例也能对预测起作用，估计误差较小，但近似误差较大，整体模型较为简单。 在应用中，k{\\it k}k值一般去一个比较小的数值，通常采用交叉验证法来选取最优的k{\\it k}k值。 分类决策规则 k{\\it k}k邻近法的分类决策规则往往是多数表决。 如果分类的损失函数为0−10-10−1损失函数，分类函数是 f:Rn→{c1,c2,⋯ ,cK}{\\it f:{\\bm R}^n\\rightarrow\\{c_{\\rm 1}, c_{\\rm 2}, \\cdots, c_K\\}} f:Rn→{c1​,c2​,⋯,cK​} 误分类的概率为 P(Y≠f(X))=1−P(Y=f(X)){\\it P(Y {\\rm \\neq} f(X))}= 1-{\\it P(Y = f(X))} P(Y=f(X))=1−P(Y=f(X)) 于是对于给定的实例x∈X{\\it x}\\in\\mathcal{X}x∈X，误分类的概率为 1k∑xi∈Nk(x)I(yi≠cj)=1−1k∑xi∈Nk(x)I(yi=cj)\\frac{1}{\\it k}\\sum_{\\it x_i \\in N_k(x)}{\\it I(y_i\\neq c_j)} = 1 - \\frac{1}{\\it k}\\sum_{\\it x_i \\in N_k(x)}{\\it I(y_i=c_j)} k1​xi​∈Nk​(x)∑​I(yi​=cj​)=1−k1​xi​∈Nk​(x)∑​I(yi​=cj​) 要使误分类率最小即经验风险最小，就要使∑xi∈Nk(x)I(yi=cj)\\sum \\limits_{\\it x_i \\in N_k(x)}{\\it I(y_i=c_j)}xi​∈Nk​(x)∑​I(yi​=cj​)最大，所以多数表决规则等价于经验风险最小化。 3.3 k{\\it k}k近邻算法的实现：kd{\\it kd}kd树 实现k{\\it k}k近邻法需要考虑如何进行快速的k{\\it k}k近邻搜索。特征空间维数大和训练数据容量大时尤为重要。 最简单的方法是线性扫描（linear scan），即将输入实例与每个训练实例逐一进行计算，这种方法非常耗时。 故介绍kd{\\it kd}kd树方法来减少计算距离的次数。 构造kd{\\it kd}kd树 kd{\\it kd}kd树是一种对k{\\it k}k维空间中的实例点进行存储以便对其进行快速检索的二叉树。 构造方法如下： 构造根结点：根结点对应于包含整个训练集的k{\\it k}k维空间超矩形区域。 切分：选择第一维为坐标轴，以所有训练实例中第一维的坐标中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。由根节点生成深度为111的左、右子结点。左子结点对应第一维坐标小于切分点的子区域，右子结点对应第一维坐标大于切分点的子区域。落在切分点上的实例保存在根节点中。 重复：对于深度为j{\\it j}j的结点，选择第l{\\it l}l维作为切分的坐标轴并进行切分，其中l=j(mod k)+1{\\it l=j({\\rm mod}~k)+}1l=j(mod k)+1。重复该步骤。 结束：当两个子区域没有实例存在时停止，kd{\\it kd}kd树的区域划分形成。 搜索kd{\\it kd}kd树 给定目标点后，搜索其最近邻。首先找包含目标点的叶结点，然后从叶结点出发依次回退到父结点，不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。 搜索方法如下： 从根结点出发，递归地向下访问kd{\\it kd}kd树。如果目标点x{\\it x}x当前维度的坐标小于切分点则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止。 以该叶结点为“当前最近点”。 递归地向上回退，在每个结点进行如下操作： a. 如果该结点比当前最近点更近，那么设该点为“当前最近点” b. 当前最近点一定在该结点的一个子结点处，检查该子结点的兄弟结点内是否有更近的点。具体地，以目标点为球心，到“当前最近点”为半径，检查超球体是否和另一子结点的对应区域相交。如果相交，则可能存在更近的点，存在的话就移动至更近点。如果不相交则继续向上回退。 回退到根结点，搜索结束。最后的“当前最近点”即为x{\\it x}x的最近邻点。 该算法可用于求k{\\it k}k近邻。 如果实例点随机分布，kd{\\it kd}kd树搜索的平均计算复杂度是O(log⁡N){\\it O(\\log N)}O(logN)，N{\\it N}N为训练实例总数。kd{\\it kd}kd适合用于训练实例数远大于空间维数时的k{\\it k}k近邻搜索。 当空间维数接近训练实例时，该方法的效率会迅速退化至接近线性扫描。 笔者对kd{\\it kd}kd树的理解是一种基于类似二分法操作的数据结构，只不过二分法中只有一个维度。kd{\\it kd}kd树则每次切换一个不同的维度，尽可能地平分实例点（寻找中位数），平衡二叉树，依照不同维度将数据划分到log⁡2N\\log_2{\\it N}log2​N高度的树中。在不断从叶结点回退至根结点的过程中，不停地检查本结点以及邻居结点内是否存在更近点，从而实现在O(log⁡N){\\it O(\\log N)}O(logN)的计算复杂度内完成对k{\\it k}k近邻的搜索。 习题 3.1 k{\\it k}k值减小时模型复杂度增大，k{\\it k}k值增大时模型复杂度减小。需要根据预测精确率选取更优的k{\\it k}k值。 3.2 (2,3) 3.3 维护一个数据结构并不断将更新的最近点存入，如果存满了则将里面距离较远的点替换为距离更近的新的最近点。为避免找到的点不足k{\\it k}k个，在超球体与邻居结点的超矩形不相交时不再跳过，而是仍然对其中的点进行距离计算。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"Tree","slug":"Tree","permalink":"https://lwdavid.github.io/tags/Tree/"}]},{"title":"《机器学习方法》 第2章 感知机","slug":"机器学习方法-第2章-感知机","date":"2023-09-12T15:51:15.000Z","updated":"2023-09-16T05:34:55.189Z","comments":true,"path":"2023/09/12/机器学习方法-第2章-感知机/","link":"","permalink":"https://lwdavid.github.io/2023/09/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC2%E7%AB%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"","text":"So Minsky and Papert were basically saying, “Frank. You’re telling us this machine is going to be conscious of its own existence but it can’t do XOR?” Shortly after the book was published, Frank Rosenblatt died. Officially, he died in a boating accident. But we all know he died of a broken heart. ——CS 189/289A Lecture 17, UC Berkeley. 1957年Rosenblatt提出感知机模型。 感知机利用分离超平面对输入的特征向量进行分类。 感知机有原始形式和对偶形式，是神经网络和SVM的基础。 2.1 感知机模型 从输入空间X⊆Rn\\mathcal{X}\\subseteq\\mathcal{R}^{\\it n}X⊆Rn到输出空间Y={+1,−1}\\mathcal{Y}=\\{+1,-1\\}Y={+1,−1}的函数 f(x)=sign(w⋅x+b){\\it f}({\\it x})={sign}({\\it w}\\cdot {\\it x}+{\\it b}) f(x)=sign(w⋅x+b) 称为感知机。其中w∈Rn{\\it w}\\in\\mathcal{R}^{\\it n}w∈Rn称为权值(weight)或权值向量(weight vector)，b∈R{\\it b}\\in\\mathcal{R}b∈R称为偏置(bias)。w⋅x{\\it w}\\cdot {\\it x}w⋅x表示w{\\it w}w和x{\\it x}x的内积，sign()sign()sign()是符号函数。 2.2 感知机学习策略 数据集的线性可分性 即对数据集中所有点，存在超平面S{\\it S}S将其正确分类。 对所有yi=+1{\\it y_i}=+1yi​=+1有w⋅x+b&gt;0{\\it w}\\cdot {\\it x}+{\\it b}&gt;0w⋅x+b&gt;0 对所有yi=−1{\\it y_i}=-1yi​=−1有w⋅x+b&lt;0{\\it w}\\cdot {\\it x}+{\\it b}&lt;0w⋅x+b&lt;0 感知机学习策略 损失函数不采用错误点的个数，因为这样不是w{\\it w}w和b{\\it b}b的连续可导函数，难以优化。 设误分类的点的集合为M{\\it M}M，则所有误分类的点到S{\\it S}S的距离的总和为： −1∥w∥∑xi∈Myi(w⋅xi+b)-\\frac{1}{\\Vert{\\it w}\\Vert}\\sum_{\\it x_i\\in M}{\\it y_i}({\\it w\\cdot x_i + b}) −∥w∥1​xi​∈M∑​yi​(w⋅xi​+b) 不考虑1∥w∥\\frac{1}{\\Vert{\\it w}\\Vert}∥w∥1​，即可得到感知机的损失函数L(w,b){\\it L(w, b)}L(w,b)： L(w,b)=−∑xi∈Myi(w⋅xi+b){\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_i}({\\it w\\cdot x_i + b}) L(w,b)=−xi​∈M∑​yi​(w⋅xi​+b) 该函数即为感知机的经验风险函数，且L(w,b){\\it L(w, b)}L(w,b)是一个非负且连续可导的函数。 2.3 感知机学习算法 原始形式 感知机学习问题已经抽象为线性可分下的最优化问题： min⁡w,bL(w,b)=−∑xi∈Myi(w⋅xi+b)\\min_{\\it w, b}{\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_i}({\\it w\\cdot x_i + b}) w,bmin​L(w,b)=−xi​∈M∑​yi​(w⋅xi​+b) 采用梯度下降法，损失函数的梯度为： ∇wL(w,b)=−∑xi∈Myixi{\\bold \\nabla}_{\\it w}{\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_ix_i} ∇w​L(w,b)=−xi​∈M∑​yi​xi​ ∇bL(w,b)=−∑xi∈Myi{\\bold \\nabla}_{\\it b}{\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_i} ∇b​L(w,b)=−xi​∈M∑​yi​ 任选一个误分类点(xi,yi)({\\it x_i, y_i})(xi​,yi​)，以其梯度为依据进行沿梯度下降方向的参数更新： w←w+ηyixi{\\it w\\leftarrow w + \\boldsymbol{\\eta} y_ix_i} w←w+ηyi​xi​ b←b+ηyi{\\it b\\leftarrow b + \\boldsymbol{\\eta} y_i} b←b+ηyi​ 式中步长η∈(0,1]\\boldsymbol{\\eta}\\in(0,1]η∈(0,1]为学习率(Learning Rate)。通过不断迭代即可将L(w,b){\\it L(w, b)}L(w,b)减小至0。 收敛性 Novikoff定理 方便起见，设w^=(w⊤,b)⊤\\hat{\\it w}=({\\it w^\\top, b})^\\topw^=(w⊤,b)⊤，x^=(x⊤,1)⊤\\hat{\\it x}=({\\it x}^\\top, 1)^\\topx^=(x⊤,1)⊤。 对于线性可分的训练数据集，存在满足条件∥w^opt∥=1\\Vert{\\hat{\\it w}_{\\rm opt}}\\Vert=1∥w^opt​∥=1的超平面w^opt⋅x^+bopt=0{\\hat{\\it w}_{\\rm opt}}\\cdot{\\hat{\\it x}+{\\it b}_{\\rm opt}}=0w^opt​⋅x^+bopt​=0将训练集完全正确分开。存在γ&gt;0\\gamma&gt;0γ&gt;0，对所有i=1,2,⋯ ,N{\\it i}=1, 2, \\cdots, {\\it N}i=1,2,⋯,N，有 yi(w^opt⋅x^i)⩾γ{\\it y_i}({\\hat{\\it w}_{\\rm opt}}\\cdot{\\hat{\\it x}_i})\\geqslant\\gamma yi​(w^opt​⋅x^i​)⩾γ 令R=max⁡1⩽i⩽N∥x^i∥{\\it R}=\\max \\limits_{\\it 1\\leqslant i \\leqslant N}\\|\\hat{\\it x}_i\\|R=1⩽i⩽Nmax​∥x^i​∥，则感知机算法在训练集上的误分类次数k{\\it k}k满足 k⩽(Rγ)2{\\it k}\\leqslant \\left(\\frac{\\it R}{\\gamma}\\right)^2 k⩽(γR​)2 证明 详细证明步骤略。 依据梯度更新步长的下界ηγ\\boldsymbol{\\eta}\\gammaηγ证明w^k⋅w^opt⩾kηγ\\hat{\\it w}_{\\it k}\\cdot\\hat{\\it w}_{opt}\\geqslant{\\it k}\\boldsymbol{\\eta}\\gammaw^k​⋅w^opt​⩾kηγ。 根据完全平方式得出权值变化的上界η2R2\\boldsymbol{\\eta}^2{\\it R}^2η2R2，证明∥w^k∥⩽kη2R2\\|\\hat{\\it w}_{\\it k}\\|\\leqslant{\\it k}\\boldsymbol{\\eta}^2{\\it R}^2∥w^k​∥⩽kη2R2。 综合以上两式，证明结论k⩽(Rγ)2{\\it k}\\leqslant \\left(\\frac{\\it R}{\\gamma}\\right)^2k⩽(γR​)2。 感知机学习算法的对偶形式 在原始形式中，注意到参数的更新规则为 w←w+ηyixi{\\it w\\leftarrow w + \\boldsymbol{\\eta} y_ix_i} w←w+ηyi​xi​ b←b+ηyi{\\it b\\leftarrow b + \\boldsymbol{\\eta} y_i} b←b+ηyi​ 也就是说，假设一个误分类点(xi,yi)({\\it x_i, y_i})(xi​,yi​)总共被误分类过ni{\\it n_i}ni​次，那么该点总共对参数w{\\it w}w和b{\\it b}b带来的更新分别是niηyixi{\\it n_i}\\boldsymbol{\\eta}{\\it y_ix_i}ni​ηyi​xi​和niηyi{\\it n_i}\\boldsymbol{\\eta}{\\it y_i}ni​ηyi​。 方便起见，令αi=niη\\boldsymbol{\\alpha}_{\\it i}={\\it n_i}\\boldsymbol{\\eta}αi​=ni​η，于是最终w{\\it w}w和b{\\it b}b的表达式可以写为： w=∑i=1Nαiyixi{\\it w}=\\sum_{\\it i {\\rm =1}}^{\\it N}\\boldsymbol{\\alpha}_{\\it i}{\\it y_ix_i} w=i=1∑N​αi​yi​xi​ b=∑i=1Nαiyi{\\it b}=\\sum_{\\it i {\\rm =1}}^{\\it N}\\boldsymbol{\\alpha}_{\\it i}{\\it y_i} b=i=1∑N​αi​yi​ 当学习率为111时，αi\\boldsymbol{\\alpha}_{\\it i}αi​就是误分类点(xi,yi)({\\it x_i, y_i})(xi​,yi​)被分类错误并被用于修正参数的次数。 更一般地来看，αi\\boldsymbol{\\alpha}_{\\it i}αi​表明了误分类点(xi,yi)({\\it x_i, y_i})(xi​,yi​)对学习超平面参数的贡献程度。那些很少被分错的点对于确定超平面的贡献较小（约束更松弛）。而那些被分错次数较多的点则距离分离超平面更近，这些被频繁分错的点对于超平面位置的确定有更重要的贡献（约束更紧）。 此时感知机的模型可写为 f(x)=sign(∑j=1Nαjyjxj⋅x+b){\\it f(x)=}sign\\left(\\sum_{\\it j{\\rm =1}}^{\\it N}\\boldsymbol{\\alpha}_{\\it j}{\\it y_jx_j\\cdot x+b}\\right) f(x)=sign(j=1∑N​αj​yj​xj​⋅x+b) 每次选择一个数据点(xi,yi)({\\it x_i, y_i})(xi​,yi​)，如果yi(∑j=1Nαjyjxj⋅xi+b)⩽0{\\it y_i}\\left(\\sum^{\\it N} \\limits_{\\it j{\\rm =1}} \\boldsymbol{\\alpha}_{\\it j}{\\it y_jx_j\\cdot x_i+b}\\right)\\leqslant0yi​(j=1∑N​αj​yj​xj​⋅xi​+b)⩽0，则 αi←αi+η\\boldsymbol{\\alpha}_{\\it i}\\leftarrow \\boldsymbol{\\alpha}_{\\it i} + \\boldsymbol{\\eta} αi​←αi​+η b←b+ηyi{\\it b} \\leftarrow {\\it b} + \\boldsymbol{\\eta}{\\it y_i} b←b+ηyi​ 迭代直至没有误分类的数据。 由于训练实例在该过程中仅以内积&lt;xj,xi&gt;\\left&lt;{\\it x_j, x_i}\\right&gt;⟨xj​,xi​⟩的形式出现，方便起见，可以预先将内积全部计算好并以Gram矩阵的形式存储起来以备使用： G=[xi⋅xj]N×N{\\it G}=[{\\it x_i\\cdot x_j}]_{\\it N \\times N} G=[xi​⋅xj​]N×N​ 当训练数据线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。 习题 2.1 构建有四个数据点的异或运算数据集。运行感知机学习算法并验证其在该数据集上的不可收敛性，即该数据集的线性不可分性。 2.2 生成线性可分数据集并求解感知机模型。 2.3 正负实例点线性可分的充分必要条件是两种实例点构成的凸壳互不相交。 必要性： 设已知两类实例点线性可分，则存在一个超平面S:w⋅x+b=0{\\it S:w\\cdot x + b}=0S:w⋅x+b=0对于任意的一个实例点x{\\it x}x有w⋅x+b&gt;0{\\it w}\\cdot {\\it x}+{\\it b}&gt;0w⋅x+b&gt;0或者w⋅x+b&lt;0{\\it w}\\cdot {\\it x}+{\\it b}&lt;0w⋅x+b&lt;0，仅有其中一项成立。 如果该线性可分的两个凸壳S1{\\it S}_1S1​和S2{\\it S}_2S2​相交，那么实例点x∈S1∩S2{\\it x \\in S_{\\rm 1}\\cap S_{\\rm 2}}x∈S1​∩S2​既可表示为正例的凸组合，能推出w⋅x+b&gt;0{\\it w}\\cdot {\\it x}+{\\it b}&gt;0w⋅x+b&gt;0，又可表示为负例的凸组合，能推出w⋅x+b&lt;0{\\it w}\\cdot {\\it x}+{\\it b}&lt;0w⋅x+b&lt;0。两者显然矛盾，故线性可分的两类实例点构成的凸壳不可能相交。 充分性： 设已知两类实例点构成的凸壳互不相交。那么更广泛地说，对于任一实例点x∈S1∪S2{\\it x \\in S_{\\rm 1}\\cup S_{\\rm 2}}x∈S1​∪S2​，该实例仅能表示为正例的凸组合或者负例的凸组合。在两个凸壳中各自选定一点，使得两点之间距离最短，两点连线的法平面即可将数据集线性分离。能够证明，正例中的任意点均相对于该平面满足w⋅x+b&gt;0{\\it w}\\cdot {\\it x}+{\\it b}&gt;0w⋅x+b&gt;0，负例同理。因此可以证明，互不相交的两个凸壳具有线性可分性。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"},{"name":"Perceptron","slug":"Machine-Learning/Perceptron","permalink":"https://lwdavid.github.io/categories/Machine-Learning/Perceptron/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]},{"title":"《机器学习方法》 第1章 机器学习及监督学习概论","slug":"机器学习方法-第1章-机器学习及监督学习概论","date":"2023-09-09T12:56:07.000Z","updated":"2023-09-12T10:06:16.020Z","comments":true,"path":"2023/09/09/机器学习方法-第1章-机器学习及监督学习概论/","link":"","permalink":"https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC1%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/","excerpt":"","text":"1.1 统计学习 Learning is any process by which a system improves performance from experience. ——Herbert A. Simon 机器学习的对象是数据。 机器学习由监督学习、无监督学习和强化学习等组成。 机器学习的三要素是：模型、策略、算法。 实现机器学习方法的步骤是：获得数据、确定模型、选择策略、实现算法、选择模型、进行预测或分析。 1.2 机器学习的分类 基本分类：监督学习、无监督学习、强化学习、半监督学习与主动学习。 给出了输入空间、特征空间、输出空间、实例、特征向量、联合概率分布、假设空间、状态、奖励、动作、策略、价值函数等概念的定义。 强化学习有基于策略的、基于价值的，这两种属于无模型的方法（求解策略或价值函数），此外还有有模型的方法（建模并学习问题）。 按模型分类：概率与非概率模型、线性与非线性模型、参数化与非参数化模型。 概率：决策树、朴素贝叶斯、隐马尔可夫、CRF、概率潜在语义分析、潜在狄利克雷分配、GMM。 非概率：感知机、SVM、k近邻、AdaBoost、k均值、潜在语义分析、神经网络。 线性：感知机、SVM、k近邻、k均值、潜在语义分析。 非线性：核函数SVM、AdaBoost、神经网络。 参数化：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、GMM。 非参数化：决策树、SVM、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配。 按算法分类：在线学习、批量学习 数据依次到达无法存储、系统要求及时处理、数据规模很大、模式动态变化等情况需要考虑在线学习。 按技巧分类：贝叶斯学习、核方法 贝叶斯方法：主要利用贝叶斯定理。即根据获得的数据基于后验概率去估计模型，指导模型的选择。需要选择模型时就选择后验概率最大的模型，预测时就计算整个后验概率分布上的期望。和最大似然估计的差别在于，极大似然估计直接选取后验概率最大的模型和预测，贝叶斯方法则在全体上取期望。 核方法：不显式地定义低维到高维的映射，而是直接定义映射后在特征空间的内积，也就是核函数。例如xi1{\\it x_i}_{\\rm 1}xi​1​和xi2{\\it x_i}_{\\rm 2}xi​2​的内积是&lt;xi1,xi2&gt;\\left&lt;{\\it x_i}_{\\rm 1},{\\it x_i}_{\\rm 2} \\right&gt;⟨xi​1​,xi​2​⟩，它们在特征空间的映射是φ(xi1)\\varphi({\\it x_i}_{\\rm 1})φ(xi​1​)和φ(xi2)\\varphi({\\it x_i}_{\\rm 2})φ(xi​2​)，内积是&lt;φ(xi1),φ(xi2)&gt;\\left&lt;{\\varphi}({\\it x_i}_{\\rm 1}),\\varphi({\\it x_i}_{\\rm 2}) \\right&gt;⟨φ(xi​1​),φ(xi​2​)⟩。那么核函数K(xi1,xi2){\\it K}({\\it x_i}_{\\rm 1},{\\it x_i}_{\\rm 2})K(xi​1​,xi​2​)就定义为K(xi1,xi2)=&lt;φ(xi1),φ(xi2)&gt;{\\it K}({\\it x_i}_{\\rm 1},{\\it x_i}_{\\rm 2})=\\left&lt;\\varphi({\\it x_i}_{\\rm 1}),\\varphi({\\it x_i}_{\\rm 2}) \\right&gt;K(xi​1​,xi​2​)=⟨φ(xi​1​),φ(xi​2​)⟩。 1.3 机器学习方法的三要素 方法=模型+策略+算法 模型 假设空间F\\mathcal{F}F包含所有可能的模型：F={f∣Y=f(X)}\\mathcal{F}=\\{\\it{f}|Y=f(X)\\}F={f∣Y=f(X)}. 策略 考虑按照什么准测学习或选择模型。 引入损失函数L(Y,f(xi)){\\it L}({\\it Y}, {\\it f}({\\it x_i}))L(Y,f(xi​))和风险函数Rexp(f){\\it R}_{\\rm exp}({\\it f})Rexp​(f)。 损失函数：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数（对数似然损失函数）等。 风险函数：模型的损失函数在整个联合分布P(xi,Y){\\it P}({\\it x_i}, {\\it Y})P(xi​,Y)上的期望即为风险函数，也叫期望损失。 由于整个联合分布无从得知，所以只能使用训练集上的平均损失，也就是经验风险或经验损失Remp(f){\\it R}_{\\rm emp}({\\it f})Remp​(f)。 根据大数定理，当样本容量N→∞{\\it N}\\rightarrow\\inftyN→∞时有Remp(f)→Rexp(f){\\it R}_{\\rm emp}({\\it f})\\rightarrow{\\it R}_{\\rm exp}({\\it f})Remp​(f)→Rexp​(f)。 监督学习有两个策略：经验风险最小化(Empirical Risk Minimization, ERM)和结构风险最小化(Structural Risk Minimization, SRM)。 经验风险最小化： min⁡f∈F1N∑i=1NL(yi,f(xi))\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i})) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​)) 结构风险最小化： min⁡f∈F1N∑i=1NL(yi,f(xi))+λJ(f)\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i}))+\\lambda{\\it J}({\\it f}) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​))+λJ(f) 结构风险最小化在经验风险最小化的基础上添加一个正则化项（罚项）以控制模型的复杂度。 算法 即求解最优化问题的具体计算方法。 1.4 模型评估与模型选择 训练误差、测试误差。 模型的选择：过拟合（模型复杂度高，测试误差与训练误差重新形成较大差距）。 1.5 正则化与交叉验证 正则化项可采用Lp{\\it L}_{\\rm p}Lp​-范数： ∥xi∥p=(∑i=1n∣xii∣p)1p\\lVert{\\it x_i}\\rVert_p=\\left(\\sum^{n}_{i=1}\\lvert{\\it x_i}_{\\rm i}\\rvert^{p}\\right)^{\\frac{1}{p}} ∥xi​∥p​=(i=1∑n​∣xi​i​∣p)p1​ 交叉验证： 简单交叉验证 一部分作为训练集，一部分作为验证集。 S折交叉验证 切分为S个大小相同互不相交的子集，一个作为验证集其余作为训练集，然后重复S次。 留一交叉验证 即S=N的S折交叉验证。 1.6 泛化能力 泛化误差是该模型在X×Y\\mathcal{X}\\times\\mathcal{Y}X×Y上损失函数的期望。 对二类分类问题，可以由Hoeffding不等式推导出泛化误差上界。 1.7 生成模型与判别模型 在监督学习中， 生成模型：可学习联合概率分布、收敛快、可在样本容量增加时收敛于真实模型、能应对隐变量 朴素贝叶斯、隐马尔可夫 判别模型：直接学习决策函数或条件概率分布、准确率高、可对数据进行抽象、能定义并使用特征从而简化学习问题 k近邻、感知机、决策树、逻辑斯谛回归、最大熵模型、SVM、提升方法、CRF等 1.8 监督学习应用 分类 分类器指标：准确率(Accuracy)。 二类分类：精确率(Precision)、召回率(Recall)、F1F_1F1​分数(F1F_1F1​ score)。 常用模型：k近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、SVM、提升方法、贝叶斯网络、神经网络、Winnow等。 标注 指标与分类类似。 常用模型：隐马尔可夫、CRF。 回归 常用的损失函数是平方损失函数，此时可用最小二乘法(Least Squares)求解。 习题 1.1 极大似然估计中，模型是伯努利分布（条件概率模型），策略是ERM，算法是求导得到最大值的解析解。 贝叶斯估计中，模型也是伯努利分布（条件概率模型），策略是SRM，算法是后验概率的期望估计。 在贝叶斯估计中，对给定数据下模型不同参数的概率进行考察，其实可以考虑为对模型的正则化，即考察最有可能出现的参数。写出损失函数的表达式后可以看出其结构亦符合SRM策略。 1.2 模型是条件概率分布，损失函数是对数损失函数时，从经验风险最小化（ERM）推导极大似然估计： min⁡f∈F1N∑i=1NL(yi,f(xi))\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i})) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​)) =min⁡f∈F1N∑i=1N(−log⁡p(yi∣xi))=\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}\\left(-\\log{\\it p}({\\it y_i}\\vert{\\it x_i})\\right) =f∈Fmin​N1​i=1∑N​(−logp(yi​∣xi​)) =max⁡f∈F1N∑i=1N(log⁡p(yi∣xi))=\\max_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}\\left(\\log{\\it p}({\\it y_i}\\vert{\\it x_i})\\right) =f∈Fmax​N1​i=1∑N​(logp(yi​∣xi​)) =max⁡f∈F1N∏i=1Np(yi∣xi)=\\max_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\prod^{\\it N}_{i=1}{\\it p}({\\it y_i}\\vert{\\it x_i}) =f∈Fmax​N1​i=1∏N​p(yi​∣xi​) =1Nmax⁡f∈FL(Y∣X)=\\frac{1}{\\it N}\\max_{\\it f\\in\\mathcal{F}} {\\it L}({\\it Y}\\vert{\\it X}) =N1​f∈Fmax​L(Y∣X) 结果即为最大似然估计法。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"},{"name":"Perceptron","slug":"Machine-Learning/Perceptron","permalink":"https://lwdavid.github.io/categories/Machine-Learning/Perceptron/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"Tree","slug":"Tree","permalink":"https://lwdavid.github.io/tags/Tree/"}]}