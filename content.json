{"meta":{"title":"Site","subtitle":"","description":"","author":"David","url":"https://lwdavid.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2023-09-09T15:43:03.237Z","updated":"2023-09-09T15:43:03.237Z","comments":false,"path":"/404.html","permalink":"https://lwdavid.github.io/404.html","excerpt":"","text":""},{"title":"About","date":"2023-09-12T03:35:45.401Z","updated":"2023-09-12T03:35:45.401Z","comments":false,"path":"about/index.html","permalink":"https://lwdavid.github.io/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2023-09-09T15:47:53.563Z","updated":"2023-09-09T15:47:53.563Z","comments":false,"path":"categories/index.html","permalink":"https://lwdavid.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2023-09-09T15:42:03.619Z","updated":"2023-09-09T15:42:03.619Z","comments":false,"path":"tags/index.html","permalink":"https://lwdavid.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《机器学习方法》 第2章 感知机","slug":"机器学习方法-第2章-感知机","date":"2023-09-09T15:51:15.000Z","updated":"2023-09-12T06:09:07.974Z","comments":true,"path":"2023/09/09/机器学习方法-第2章-感知机/","link":"","permalink":"https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC2%E7%AB%A0-%E6%84%9F%E7%9F%A5%E6%9C%BA/","excerpt":"","text":"1957年Rosenblatt提出感知机模型。 感知机利用分离超平面对输入的特征向量进行分类。 感知机有原始形式和对偶形式，是神经网络和SVM的基础。 2.1 感知机模型 从输入空间X⊆Rn\\mathcal{X}\\subseteq\\mathcal{R}^{\\it n}X⊆Rn到输出空间Y={+1,−1}\\mathcal{Y}=\\{+1,-1\\}Y={+1,−1}的函数 f(x)=sign(w⋅x+b){\\it f}({\\it x})={sign}({\\it w}\\cdot {\\it x}+{\\it b}) f(x)=sign(w⋅x+b) 称为感知机。其中w∈Rn{\\it w}\\in\\mathcal{R}^{\\it n}w∈Rn称为权值(weight)或权值向量(weight vector)，b∈R{\\it b}\\in\\mathcal{R}b∈R称为偏置(bias)。w⋅x{\\it w}\\cdot {\\it x}w⋅x表示w{\\it w}w和x{\\it x}x的内积，sign()sign()sign()是符号函数。 2.2 感知机学习策略 数据集的线性可分性 即对数据集中所有点，存在超平面S{\\it S}S将其正确分类。 对所有yi=+1{\\it y_i}=+1yi​=+1有w⋅x+b&gt;0{\\it w}\\cdot {\\it x}+{\\it b}&gt;0w⋅x+b&gt;0 对所有yi=−1{\\it y_i}=-1yi​=−1有w⋅x+b&lt;0{\\it w}\\cdot {\\it x}+{\\it b}&lt;0w⋅x+b&lt;0 感知机学习策略 损失函数不采用错误点的个数，因为这样不是w{\\it w}w和b{\\it b}b的连续可导函数，难以优化。 设误分类的点的集合为M{\\it M}M，则所有误分类的点到S{\\it S}S的距离的总和为： −1∥w∥∑xi∈Myi(w⋅xi+b)-\\frac{1}{\\Vert{\\it w}\\Vert}\\sum_{\\it x_i\\in M}{\\it y_i}({\\it w\\cdot x_i + b}) −∥w∥1​xi​∈M∑​yi​(w⋅xi​+b) 不考虑1∥w∥\\frac{1}{\\Vert{\\it w}\\Vert}∥w∥1​，即可得到感知机的损失函数L(w,b){\\it L(w, b)}L(w,b)： L(w,b)=−∑xi∈Myi(w⋅xi+b){\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_i}({\\it w\\cdot x_i + b}) L(w,b)=−xi​∈M∑​yi​(w⋅xi​+b) 该函数即为感知机的经验风险函数，且L(w,b){\\it L(w, b)}L(w,b)是一个非负且连续可导的函数。 2.3 感知机学习算法 原始形式 感知机学习问题已经抽象为最优化问题： min⁡w,bL(w,b)=−∑xi∈Myi(w⋅xi+b)\\min_{\\it w, b}{\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_i}({\\it w\\cdot x_i + b}) w,bmin​L(w,b)=−xi​∈M∑​yi​(w⋅xi​+b) 采用梯度下降法，损失函数的梯度为： ∇wL(w,b)=−∑xi∈Myixi{\\bold \\nabla}_{\\it w}{\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_ix_i} ∇w​L(w,b)=−xi​∈M∑​yi​xi​ ∇bL(w,b)=−∑xi∈Myi{\\bold \\nabla}_{\\it b}{\\it L(w, b)}=-\\sum_{\\it x_i\\in M}{\\it y_i} ∇b​L(w,b)=−xi​∈M∑​yi​ 任选一个误分类点(xi,yi)({\\it x_i, y_i})(xi​,yi​)，以其梯度为依据进行沿梯度下降方向的参数更新： w←w+ηηyixi{\\it w\\leftarrow w+ \\eta \\mathit{\\eta} y_ix_i} w←w+ηηyi​xi​ 习题 katex.render(&quot;c=pmsqrta2+b2&quot;,element,throwOnError:false);katex.render(&quot;c = \\\\pm\\\\sqrt{a^2 + b^2}&quot;, element, {throwOnError: false});katex.render(&quot;c=pmsqrta2+b2&quot;,element,throwOnError:false);","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"},{"name":"Perceptron","slug":"Machine-Learning/Perceptron","permalink":"https://lwdavid.github.io/categories/Machine-Learning/Perceptron/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]},{"title":"《机器学习方法》 第1章 机器学习及监督学习概论","slug":"机器学习方法-第1章-机器学习及监督学习概论","date":"2023-09-09T12:56:07.000Z","updated":"2023-09-12T04:07:04.837Z","comments":true,"path":"2023/09/09/机器学习方法-第1章-机器学习及监督学习概论/","link":"","permalink":"https://lwdavid.github.io/2023/09/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E7%AC%AC1%E7%AB%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/","excerpt":"","text":"1.1 统计学习 学习的定义: Learning is any process by which a system improves performance from experience. ——Herbert A. Simon 机器学习的对象是数据。 机器学习由监督学习、无监督学习和强化学习等组成。 机器学习的三要素是：模型、策略、算法。 实现机器学习方法的步骤是：获得数据、确定模型、选择策略、实现算法、选择模型、进行预测或分析。 1.2 机器学习的分类 基本分类：监督学习、无监督学习、强化学习、半监督学习与主动学习。 给出了输入空间、特征空间、输出空间、实例、特征向量、联合概率分布、假设空间、状态、奖励、动作、策略、价值函数等概念的定义。 强化学习有基于策略的、基于价值的，这两种属于无模型的方法（求解策略或价值函数），此外还有有模型的方法（建模并学习问题）。 按模型分类：概率与非概率模型、线性与非线性模型、参数化与非参数化模型。 概率：决策树、朴素贝叶斯、隐马尔可夫、CRF、概率潜在语义分析、潜在狄利克雷分配、GMM。 非概率：感知机、SVM、k近邻、AdaBoost、k均值、潜在语义分析、神经网络。 线性：感知机、SVM、k近邻、k均值、潜在语义分析。 非线性：核函数SVM、AdaBoost、神经网络。 参数化：感知机、朴素贝叶斯、逻辑斯谛回归、k均值、GMM。 非参数化：决策树、SVM、AdaBoost、k近邻、潜在语义分析、概率潜在语义分析、潜在狄利克雷分配。 按算法分类：在线学习、批量学习 数据依次到达无法存储、系统要求及时处理、数据规模很大、模式动态变化等情况需要考虑在线学习。 按技巧分类：贝叶斯学习、核方法 贝叶斯方法：主要利用贝叶斯定理。即根据获得的数据基于后验概率去估计模型，指导模型的选择。需要选择模型时就选择后验概率最大的模型，预测时就计算整个后验概率分布上的期望。和最大似然估计的差别在于，极大似然估计直接选取后验概率最大的模型和预测，贝叶斯方法则在全体上取期望。 核方法：不显式地定义低维到高维的映射，而是直接定义映射后在特征空间的内积，也就是核函数。例如xi1{\\it x_i}_{\\rm 1}xi​1​和xi2{\\it x_i}_{\\rm 2}xi​2​的内积是&lt;xi1,xi2&gt;\\left&lt;{\\it x_i}_{\\rm 1},{\\it x_i}_{\\rm 2} \\right&gt;⟨xi​1​,xi​2​⟩，它们在特征空间的映射是φ(xi1)\\varphi({\\it x_i}_{\\rm 1})φ(xi​1​)和φ(xi2)\\varphi({\\it x_i}_{\\rm 2})φ(xi​2​)，内积是&lt;φ(xi1),φ(xi2)&gt;\\left&lt;{\\varphi}({\\it x_i}_{\\rm 1}),\\varphi({\\it x_i}_{\\rm 2}) \\right&gt;⟨φ(xi​1​),φ(xi​2​)⟩。那么核函数K(xi1,xi2){\\it K}({\\it x_i}_{\\rm 1},{\\it x_i}_{\\rm 2})K(xi​1​,xi​2​)就定义为K(xi1,xi2)=&lt;φ(xi1),φ(xi2)&gt;{\\it K}({\\it x_i}_{\\rm 1},{\\it x_i}_{\\rm 2})=\\left&lt;\\varphi({\\it x_i}_{\\rm 1}),\\varphi({\\it x_i}_{\\rm 2}) \\right&gt;K(xi​1​,xi​2​)=⟨φ(xi​1​),φ(xi​2​)⟩。 1.3 机器学习方法的三要素 方法=模型+策略+算法 模型 假设空间F\\mathcal{F}F包含所有可能的模型：F={f∣Y=f(X)}\\mathcal{F}=\\{\\it{f}|Y=f(X)\\}F={f∣Y=f(X)}. 策略 考虑按照什么准测学习或选择模型。 引入损失函数L(Y,f(xi)){\\it L}({\\it Y}, {\\it f}({\\it x_i}))L(Y,f(xi​))和风险函数Rexp(f){\\it R}_{\\rm exp}({\\it f})Rexp​(f)。 损失函数：0-1损失函数、平方损失函数、绝对损失函数、对数损失函数（对数似然损失函数）等。 风险函数：模型的损失函数在整个联合分布P(xi,Y){\\it P}({\\it x_i}, {\\it Y})P(xi​,Y)上的期望即为风险函数，也叫期望损失。 由于整个联合分布无从得知，所以只能使用训练集上的平均损失，也就是经验风险或经验损失Remp(f){\\it R}_{\\rm emp}({\\it f})Remp​(f)。 根据大数定理，当样本容量N→∞{\\it N}\\rightarrow\\inftyN→∞时有Remp(f)→Rexp(f){\\it R}_{\\rm emp}({\\it f})\\rightarrow{\\it R}_{\\rm exp}({\\it f})Remp​(f)→Rexp​(f)。 监督学习有两个策略：经验风险最小化(Empirical Risk Minimization, ERM)和结构风险最小化(Structural Risk Minimization, SRM)。 经验风险最小化： min⁡f∈F1N∑i=1NL(yi,f(xi))\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i})) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​)) 结构风险最小化： min⁡f∈F1N∑i=1NL(yi,f(xi))+λJ(f)\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i}))+\\lambda{\\it J}({\\it f}) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​))+λJ(f) 结构风险最小化在经验风险最小化的基础上添加一个正则化项（罚项）以控制模型的复杂度。 算法 即求解最优化问题的具体计算方法。 1.4 模型评估与模型选择 训练误差、测试误差。 模型的选择：过拟合（模型复杂度高，测试误差与训练误差重新形成较大差距）。 1.5 正则化与交叉验证 正则化项可采用Lp{\\it L}_{\\rm p}Lp​-范数： ∥xi∥p=(∑i=1n∣xii∣p)1p\\lVert{\\it x_i}\\rVert_p=\\left(\\sum^{n}_{i=1}\\lvert{\\it x_i}_{\\rm i}\\rvert^{p}\\right)^{\\frac{1}{p}} ∥xi​∥p​=(i=1∑n​∣xi​i​∣p)p1​ 交叉验证： 简单交叉验证 一部分作为训练集，一部分作为验证集。 S折交叉验证 切分为S个大小相同互不相交的子集，一个作为验证集其余作为训练集，然后重复S次。 留一交叉验证 即S=N的S折交叉验证。 1.6 泛化能力 泛化误差是该模型在X×Y\\mathcal{X}\\times\\mathcal{Y}X×Y上损失函数的期望。 对二类分类问题，可以由Hoeffding不等式推导出泛化误差上界。 1.7 生成模型与判别模型 在监督学习中， 生成模型：可学习联合概率分布、收敛快、可在样本容量增加时收敛于真实模型、能应对隐变量 朴素贝叶斯、隐马尔可夫 判别模型：直接学习决策函数或条件概率分布、准确率高、可对数据进行抽象、能定义并使用特征从而简化学习问题 k近邻、感知机、决策树、逻辑斯谛回归、最大熵模型、SVM、提升方法、CRF等 1.8 监督学习应用 分类 分类器指标：准确率(Accuracy)。 二类分类：精确率(Precision)、召回率(Recall)、F1F_1F1​分数(F1F_1F1​ score)。 常用模型：k近邻、感知机、朴素贝叶斯、决策树、决策列表、逻辑斯谛回归、SVM、提升方法、贝叶斯网络、神经网络、Winnow等。 标注 指标与分类类似。 常用模型：隐马尔可夫、CRF。 回归 常用的损失函数是平方损失函数，此时可用最小二乘法(Least Squares)求解。 习题 1.1 极大似然估计中，模型是伯努利分布（条件概率模型），策略是ERM，算法是求导得到最大值的解析解。 贝叶斯估计中，模型也是伯努利分布（条件概率模型），策略是SRM，算法是后验概率的期望估计。 在贝叶斯估计中，对给定数据下模型不同参数的概率进行考察，其实可以考虑为对模型的正则化，即考察最有可能出现的参数。写出损失函数的表达式后可以看出其结构亦符合SRM策略。 1.2 模型是条件概率分布，损失函数是对数损失函数时，从经验风险最小化（ERM）推导极大似然估计： min⁡f∈F1N∑i=1NL(yi,f(xi))\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}{\\it L}({\\it y_i}, {\\it f}({\\it x_i})) f∈Fmin​N1​i=1∑N​L(yi​,f(xi​)) =min⁡f∈F1N∑i=1N(−log⁡p(yi∣xi))=\\min_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}\\left(-\\log{\\it p}({\\it y_i}\\vert{\\it x_i})\\right) =f∈Fmin​N1​i=1∑N​(−logp(yi​∣xi​)) =max⁡f∈F1N∑i=1N(log⁡p(yi∣xi))=\\max_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\sum^{\\it N}_{i=1}\\left(\\log{\\it p}({\\it y_i}\\vert{\\it x_i})\\right) =f∈Fmax​N1​i=1∑N​(logp(yi​∣xi​)) =max⁡f∈F1N∏i=1Np(yi∣xi)=\\max_{\\it f\\in\\mathcal{F}} \\frac{1}{\\it N} \\prod^{\\it N}_{i=1}{\\it p}({\\it y_i}\\vert{\\it x_i}) =f∈Fmax​N1​i=1∏N​p(yi​∣xi​) =1Nmax⁡f∈FL(Y∣X)=\\frac{1}{\\it N}\\max_{\\it f\\in\\mathcal{F}} {\\it L}({\\it Y}\\vert{\\it X}) =N1​f∈Fmax​L(Y∣X) 结果即为最大似然估计法。","categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]}],"categories":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/categories/Machine-Learning/"},{"name":"Perceptron","slug":"Machine-Learning/Perceptron","permalink":"https://lwdavid.github.io/categories/Machine-Learning/Perceptron/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://lwdavid.github.io/tags/Machine-Learning/"},{"name":"机器学习方法","slug":"机器学习方法","permalink":"https://lwdavid.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]}